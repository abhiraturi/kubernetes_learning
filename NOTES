Day 1:
 1018 kubectl get nodes
 1019  pwd
 1020  vim config.yaml
 1021  ll
 1022  ls -lrt
 1023  clear
 1024  kind create cluster --name cka-cluster2 --config config.yaml
 1025  kubectl get nodes
 1027   kubectl config get-contexts
 1028   kubectl config set-contexts kind-cka-cluster1 
 1029   kubectl config use-context kind-cka-cluster1 
 1030  kubectl get nodes
 1031   kubectl config use-context kind-cka-cluster2
 1032  kubectl get nodes

Day 2:
1060  kubectl run nginx --image=ngingx --dry-run=client
 1061  kubectl run nginx --image=ngingx --dry-run=client -o yaml
 1062  kubectl run nginx --image=ngingx --dry-run=client -o yaml > pod_new.yaml
 1063  ll
 1064  ls -lrt
 1065  cat pod_new.yaml
 1066  kubectl run nginx --image=ngingx --dry-run=client -o json > pod_new.json
 1067  ls -lrt
 1068  cat pod_new.json
 1069  kubectl get pods nginx-pod --show-labels
 1070  kubectl get pods nginx-pod -o wide
 1071  kubectl get pods  -o wide
 1072  kubectl get namespaces
 1073  kubectl get ns
 1074  kubectl get nodes
 1075  kubectl get nodes -o wide
kubectl describe pod nginx-ppod
kubectl delete pod nginx-ppod
kubectl create -f pods.yaml
kubectl exec -it nginx-pod -- /bin/sh


Day 3: Replication controller (replica set is the newer version, use replicates instead of replication controller now)
So,  with replicates you can manage the already running pods under the replicates using labels

1030  kubectl get po
 1031  kubectl delete po nginx-4h5md nginx-dbkm8 nginx-rzrkw
 1032  kubectl get po
 1033  kubectl get rs
 1034  kubectl get rc
 1035  kubectl delete rc nginx
 1036  kubectl get po
 1037  kubectl apply -f rc.yaml
 1038  vi rc.yaml
 1039  kubectl apply -f rc.yaml
 1040  kubectl get rs
 1041  kubectl get po
 1042  kubectl edit rs nginx
 1043  kubectl get po
 1044  kubectl get rs
 1045  kubectl delete rs/nginx
Kubectl delete rs nginx
Kubectl explain rs

Day 4: Deployment:
Deployment manages replicates and replicates manages pods. Deployment provides extra functionality.
e.g. if we want to upgrade image of nginx, if you do it using replicas set you will face downtime.
So, deployment makes changes in rolling manner, so there is no downtime.
Deployment creates  the replicaset and the replicaset creates the pods
So, if you want to change the image, you can do from below ways
Kubectl set image deploy/nginx-deploy nginx=nginx:1.9.1
Or directly edit the yaml file and apply 
Or directly update the deploy yaml manifest using command -> kubectl edit deployment/nginx-deploy

 1067  kubectl describe po nginx-deploy-7fff95c694-nvzng
 1068  kubectl describe deploy/nginx-deploy
 1069  kubectl describe deployment/nginx-deploy
 1070  kubectl describe deploy/nginx-deploy
 1073  kubectl rollout history deployment/nginx-deploy
 1074  kubectl rollout undo deployment/nginx-deploy
 1075  kubectl describe deploy/nginx-deploy
 1076  kubectl create deployment/nginx-deploy --dry-run=client
 1079  kubectl create deployment nginx-deploy --image=nginx --dry-run=client -o yaml
 1080  kubectl create deployment nginx-deploy --image=nginx --dry-run=client -o yaml > deployment.yaml
Kubectl explain deployment
Kubectl get all
Kubectl get po -o wide
kubectl delete deployment nginx-deploy


Day 5: services
We want to access this nginx frontend external, this can be done using services.
So, to make the application accessible to users and also to make the sure that the frontend pods are able to communicate with backend pods and external data sources we use services.

Users -> service -> frontend pods -> service -> backend pods -> service -> database

Different types of services:
1. Cluster ip
2. 2. Nodeport
3. External names
4. Load balancer


1049  kubect get svc
 1050  kubectl get svc
 1051  kubectl describe svc nodeport-svc
 1052  curl localhost:30001
 1053  curl localhost:3001


Day 6:  namespaces:
1115  k get deployment -n demo
 1116  k get po -n demo
 1117  k exec -it nginx-demo-87cd4cbb7-v7w8d -n demo /bin/bash
 1118  k get po -n demo -o wide
 1119  clear
 1120  k get deployment -n demo
 1121  k scale --replicas=3 deployment nginx-demo -n demo
 1122  k get deployment -n demo
 1123  k expose svc-demo deployment nginx-demo --port 80 -n demo
 1124  k expose deployment nginx-demo --name=svc-demo --port 80 -n demo
 1125  k get svc -n demo
 1126  k get po -n demo
 1127  k exec -it nginx-demo-87cd4cbb7-2bb6d /bin/sh
 1128  k exec -it nginx-demo-87cd4cbb7-2bb6d /bin/bash
 1129  k exec -it nginx-demo-87cd4cbb7-2bb6d -- /bin/bash
 1130  k exec -it nginx-demo-87cd4cbb7-2bb6d -n demo  /bin/bash


Day 7 : Multi container pod kubernetes
Pod will show up and running if all the init containers in that pod is running.

 1193  k get svc
 1194  k delete svc mydb
 1195  k get po
 1196  k delete po my-app
 1197  k get po
 1198  k delete po my-db-65776cb9c7-kh4m2 
 1199  k get po
 1200  k create -f multicontainerpod.yaml
 1201  k get po
 1202  k get svc
 1203  k get deployment
 1204  k get po
 1205  k get deployment
 1206  k expose deployment my-db  --name mydb --port 80
 1207  k get svc
 1208  k get po

Day 8 : Daemonset/cron job
Daemonset e.g. monitoring, logs, etc, only one runs per node.

1221  k apply -f daemonset.yaml
 1222  k get daemonset
 1223  k get nodes
 1224  k get po
 1225  k get daemonset -n kube-system


Day 9 : Manual scheduloing/ selectors/ labels
scheduler does not manage static pod but kubelet manages the static pods.
Pods can be scheduled manually by mentioning the node in the pods yaml file. So, even if the scheduler is down, the pods will still be scheduled in the node mentioned in the yaml file.
generally, the scheduler, api server, controller manager, etc yamls are under /etc/kubernetes/manifest directory in control plane node, these are called static pods.
This video is about scheduling and selecting pods in Kubernetes. The video starts with an introduction of the Kubernetes architecture and the role of the scheduler in assigning pods to nodes. The video then introduces the concept of static pods, which are control plane components that are not managed by the scheduler. The video also covers manual scheduling, which allows users to specify the node on which a pod should be scheduled. Finally, the video discusses labels and selectors, which are used to filter and group resources in Kubernetes.

 1257  k get ns
 1258  k get po --selector tier=backend -n demo
 1259  k get po
 1260  k describe po redis 
 1261  k get po --selector type=backend -n demo


Day 10: taints and tolerations
We are instructing node to only accept specific pod that has the tolerations.
e.g. we want to run AI pod on a specific node because that node has specifications to run the AI workload. SO based on tolerations matching the AI pods will be acepted by node and rest of the other pods will be rejected and scheduled on dfifferent node.
Taint is done on node and toleration is done on pods.

types:
noschedule : works only for newer pods
noexecute: works for all existing and newer pods
prefernoschedule: no guarantee of pods scheduling for the taint and tolerations.

selector:
We add the label on node and pod, so it will schedule the pod to those nodes that has same labels. So, it will not go to the node that does not have matching labels as that of the pod.

So, taints and tolerations is to avoid the nodes but labels and selector tells which pod will be scheduled on what node.

k taint node cka-cluster3-worker gpu=true:NoSchedule
node/cka-cluster3-worker tainted
1322  k delete po nginx redis
 1323  k get po
 1324  k run nginx-new --image=nginx --dry-run=client -o yaml
 1325  k run nginx-new --image=nginx --dry-run=client -o yaml > newnginx.yaml
 1326  k apply -f newnginx.yaml
 1327  k get po
 1328  k get po
 1329  k get nodes --show-labels
 1330  k get nodes --show-labels
 1331  k get po
 1332  k get nodes 
 1333  k label node cka-cluster3-worker2  gpu=false
 1334  k get nodes --show-labels
 1335  k get po




 Day 11: Node Affinity:
It can have multiple labels attached to nodes e.g. disk in (ssd,hdd) which is not possible in taints and tolerations.

types:
1. requiredDuringSchedulingIgnoredDuringExecution
the labels must match otherwise pods will not be schduled, pods will get stuck in pending state if affinity does not match.
If pur node is priority that a pod needs to be scheduled on a specific node then go with this.

2. preferredDuringSchedulingIgnoredDuringExecution
first check the labels , if matches schedule the pod otherwise just schedule the pod. If pod is out priority i.e. the pod must be scheduled irrespective of affinity matching or not, then use this.

 1351  k apply -f affinity.yaml
 1352  k apply -f affinity.yaml
 1353  k get po
 1354  k get node --show-labels
 1355  k get node
 1356  k label node cka-cluster3-worker2 disktype=ssd
 1357  k get po
 1358  k get po
 1359  k label node cka-cluster3-worker2 diskType=ssd
 1360  k get po
 1361  k apply -f affinity2.yaml
 1362  k apply -f affinity2.yaml
 1363  k apply -f affinity2.yaml
 1364  k apply -f affinity2.yaml
 1365  k get po

Day 12: Resources limits and requests
CPU throttling: when pod go past the cpu limits, its cpu throttling. So, it can go beyond available CPU called cpu throttling.
OOM: When pod go past the memory limits, its gives OOM errors.

We specify requests and limits in container so that we can restrict the pod to get extra resources and the pod will crash when a limit is defined and it goes past that. In this way the pod will not occupy entire resources of the node which can hamper othe r pods scheduling on same node.
Instead of failing the node, its better to fail the pod.

Metric server:
metric server that exposes the metrics of the nodes e.g. cpu, memory utilization to the processes for autosaling etc.
It creates extra metrics pod under kube-system in control plane.
1421  k delete po metrics-server-db4f45b97-vf7gg -n kube-system
 1422  k get po -n kube-system | grep metric
 1423  k get po -n kube-system | grep metric
 1424  k get po -n kube-system | grep metric
 1425  k top node
 1426  clear
 1427  k create ns mem-example
 1428  k get ns
 1429  k top pod
 1430  k top pod -n kube-system
 1433  k top pod -n kube-system | sort -n
 1434  k get events -n kube-system
 1435  clear
 1436  k get nodes


Day 13:Autoscaling:
Application with 1000 of pods, its not possible to scale manually.
Based on resources utillization we can autoscale.

HPA: horizontal pod Autoscaling. Also called scale out/sclae in
This will add identical pods on deployment based on load in the server based on cpu, memory.
There are more users, it will add more replicas. We are adding pods horizontally and automatically.
This can be applied on two different aspects 1. infra i.e. nodes 2. worklloads i.e. pods

VPA: vertical pod automatically
Increase the pod size instead of adding. The pod will be replaced by bigger pod. stateless in nature, downtime will be there.
This can be applied on two different aspects 1. infra i.e. nodes 2. worklloads i.e. pods

Event based autoscaling (keda)

Only hpa comes as default as part of kubernetes.

 1446  k get po
 1447  k get svc
 1449  k autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
 1450  k get po
 1451  k get po
 1452  k get hpa
 1458  k get hpa --watch

Day 14: health probes
Health checks using http/tcp in an interval of time.

Three types:
1. startup probes -> for slow legacy apps. first startup probe will be active and only then readiness and liveness probes gets active.
for production, combination of probes are used.
2. readiness probes -> ensure the application is ready before the application starts serving traffic to the user.
3. liveness probes -> restart the application if it fails
it actually monitors the application, if application fails or crash due to network etc, the liveness probes restarts the pod.

So, when probe fails continuously for sometime, you will see crashloopbackoff for the pod.


Day 15: Configmap and secrets
When env variable grows, not a good practise to keep in different yaml.
We create configmap and key value inside configmap and inject those inside pods.
best practise is to use volume mount inconfig map.
1561  k get cm
 1562  k describe cm/app-cm
 1563  clear
 1564  k get po
 1565  k apply -f pod.yaml
 1566  k get po
 1567  k describe po myapp-pod
 1568  k exed -it  myapp-pod -- /bin/bash
 1569  k exec -it  myapp-pod -- /bin/bash
 1570  k exec -it  myapp-pod -- /bin/sh
 1571  k create cm app-cm --from-file=app.config
 1572  k create cm app-cm --from-literal=firstname=abhishek \\n--from-literal=lastname=raturi -o yaml
 1573  k create cm app-cm --from-literal=firstname=abhishek \\n--from-literal=lastname=raturi --dry-run=client -o yaml
 1574  k create cm app-cm --from-literal=firstname=abhishek \\n--from-literal=lastname=raturi --dry-run=client -o yaml > cm.yaml